{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPhrIbukPok8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Specify the semicolon as the delimiter\n",
        "        temp_data = pd.read_csv(file_path, sep=';')\n",
        "        all_data = pd.concat([all_data, temp_data], ignore_index=True)\n",
        "\n",
        "# Display the combined dataset\n",
        "print(\"Combined DataFrame:\")\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "LeMe2snTPvee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one of the CSV files for inspection\n",
        "file_path = '/content/drive/My Drive/Preprocessed/HUPA0015P.csv'  # Replace with the actual path\n",
        "data = pd.read_csv(file_path, sep=';', header=None)  # Use sep=';' to handle semicolon delimiter\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "3IiezN7LPyLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path, sep=';', header=0, engine='python')  # Use 'python' engine for better compatibility\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "9ftPassWPz-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read and split the data\n",
        "        temp_data = pd.read_csv(file_path, header=None)\n",
        "        temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "        temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "        # Combine into the main DataFrame\n",
        "        all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "JB5XDwaFP2PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "# Handle missing values\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'  # Replace with your actual target column name\n",
        "X = all_data.drop(columns=[target_column, 'time'])  # Drop 'time' column from features\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, errors='coerce' to handle potential non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "b6m6twHdP4z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the MLP model\n",
        "mlp_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "mlp_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "mlp_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_mlp = mlp_model.predict(X_test)\n",
        "\n",
        "# Define thresholds for categorization\n",
        "thresholds = [70, 140]  # Example thresholds\n",
        "\n",
        "# Categorize predictions and true values\n",
        "y_test_cat = np.digitize(y_test, thresholds)\n",
        "predictions_mlp_cat = np.digitize(predictions_mlp, thresholds)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_mlp = confusion_matrix(y_test_cat, predictions_mlp_cat)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "precision_mlp = precision_score(y_test_cat, predictions_mlp_cat, average='weighted')\n",
        "recall_mlp = recall_score(y_test_cat, predictions_mlp_cat, average='weighted')\n",
        "accuracy_mlp = accuracy_score(y_test_cat, predictions_mlp_cat)\n",
        "f1_mlp = f1_score(y_test_cat, predictions_mlp_cat, average='weighted')\n",
        "\n",
        "# Print metrics\n",
        "print(\"MLP Metrics:\")\n",
        "print(\"Precision:\", precision_mlp)\n",
        "print(\"Recall:\", recall_mlp)\n",
        "print(\"Accuracy:\", accuracy_mlp)\n",
        "print(\"F1-score:\", f1_mlp)"
      ],
      "metadata": {
        "id": "HfMG0s_zQH-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_Pf6Yb8QMZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}